{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/human71/comp-mt/main/head.png\" width=30%><h1><b>COMPMT: Evaluation for Machine Translation</b></h1> <h3>This notebook contains all the possible evaluation technique for machine translated files</h3>\n",
        "</center>\n",
        "\n"
      ],
      "metadata": {
        "id": "ordYFuI1ywQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 1: Automatic Evaluation Techniques**"
      ],
      "metadata": {
        "id": "Dy1xamP0t1YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install requirement packages\n",
        "!pip install -q pyter3 krippendorff"
      ],
      "metadata": {
        "id": "4HS2fjYvP2U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download all required nltk tokenizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "2PMv8SH0fWRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import required packages for Automatic Evaluation\n",
        "import pyter\n",
        "import numpy as np\n",
        "from nltk import word_tokenize\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu, sentence_bleu"
      ],
      "metadata": {
        "id": "PR5R67-EeyqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reading text from file\n",
        "ref = open(\"ref.txt\", \"r\")\n",
        "ref = ref.readlines() \n",
        "gen = open(\"gen.txt\", \"r\")\n",
        "gen = gen.readlines()\n",
        "for i,l in enumerate(gen):\n",
        "    gen[i] = l.strip()\n",
        "\n",
        "for i,l in enumerate(ref):\n",
        "    ref[i] = l.strip()"
      ],
      "metadata": {
        "id": "fDiX_vHqvbci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1.1: Function for BLEU"
      ],
      "metadata": {
        "id": "BVjwz88Zueq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bleu(ref, gen):\n",
        "    ref_bleu = []\n",
        "    gen_bleu = []\n",
        "    for l in gen:\n",
        "        gen_bleu.append(l.split())\n",
        "    for i,l in enumerate(ref):\n",
        "        ref_bleu.append([l.split()])\n",
        "    cc = SmoothingFunction()\n",
        "    score_bleu = corpus_bleu(ref_bleu, gen_bleu, weights=(0, 1, 0, 0), smoothing_function=cc.method4)\n",
        "    return score_bleu"
      ],
      "metadata": {
        "id": "Pha-tUDQPphN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1.2: Function for TER"
      ],
      "metadata": {
        "id": "Zs3dyGy5vMhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ter(ref, gen):\n",
        "    if len(ref) == 1:\n",
        "        total_score =  pyter.ter(gen[0].split(), ref[0].split())\n",
        "    else:\n",
        "        total_score = 0\n",
        "        for i in range(len(gen)):\n",
        "            total_score = total_score + pyter.ter(gen[i].split(), ref[i].split())\n",
        "        total_score = total_score/len(gen)\n",
        "    return total_score"
      ],
      "metadata": {
        "id": "_wYQrRadPtss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1.3: Function for METEOR"
      ],
      "metadata": {
        "id": "UW9yUKlKvn88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for line in zip(ref, gen):\n",
        "  refm = word_tokenize(line[0])\n",
        "  genm = word_tokenize(line[1])\n",
        "  m_score = meteor_score([refm], genm)"
      ],
      "metadata": {
        "id": "fYPus7BevTL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Printing BLEU_TER_METEOR\n",
        "print(\"BLEU = \", bleu(ref, gen))\n",
        "print(\"TER = \", ter(ref, gen))\n",
        "print(\"METEOR = \", m_score)"
      ],
      "metadata": {
        "id": "bj8JicFhQG3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 2: Human Evaluation Techniques**"
      ],
      "metadata": {
        "id": "fP-akGuGuuv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install requirement packages\n",
        "!pip install -q krippendorff"
      ],
      "metadata": {
        "id": "vUaccEK2wWg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import required packages for Automatic Evaluation\n",
        "import numpy as np\n",
        "import krippendorff as kd \n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from statsmodels.stats import inter_rater as irr"
      ],
      "metadata": {
        "id": "EpSp2Rq-wb83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialising Ratings as lists\n",
        "rater1 = [4, 3, 5, 5, 4, 4, 5, 5, 3, 4]\n",
        "rater2 = [2, 2, 5, 5,\t3, 3,\t5, 5,\t1, 1]\n",
        "rater3 = [3, 1,\t5, 5, 4, 2, 5, 5, 1, 1]\n",
        "rater4 = [3, 2, 5, 5,\t2, 2, 5, 5, 3, 2]\n",
        "ratings = [rater1, rater2, rater3, rater4]"
      ],
      "metadata": {
        "id": "c_fQa_lVyE3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2.1: Kohen Kappa Score in Between two raters"
      ],
      "metadata": {
        "id": "xKL8LKohyYmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r12 = cohen_kappa_score(rater1, rater2)\n",
        "r23 = cohen_kappa_score(rater2, rater3)\n",
        "r34 = cohen_kappa_score(rater3, rater4)\n",
        "\n",
        "print(\"RATER 01 & 02 = \", r12)\n",
        "print(\"RATER 02 & 03 = \", r23)\n",
        "print(\"RATER 03 & 04 = \", r34)"
      ],
      "metadata": {
        "id": "i6E-suOjl7YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2.2: Fleiss Kappa Score for all raters"
      ],
      "metadata": {
        "id": "0X2uUmzLyfOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f_kappa = irr.fleiss_kappa(agg[0], method='fleiss')\n",
        "print(\"Fleiss Kappa Score =\",  f_kappa)"
      ],
      "metadata": {
        "id": "N794zo86reox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2.3: Krippendorff alpha Score for all raters"
      ],
      "metadata": {
        "id": "x8A15GMPynMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_alpha = np.array(ratings).transpose()\n",
        "k_alpha = kd.alpha(k_alpha, level_of_measurement='nominal')\n",
        "print(\"Krippendorff alpha Score =\",  k_alpha)"
      ],
      "metadata": {
        "id": "S_8sfOvrsoFY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}